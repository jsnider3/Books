\documentclass{scrartcl}
\usepackage{fullpage}
\title{GDC MapReduce Notes}
\author{Josh Snider}
\begin{document}
\maketitle
\subsection*{FP Review}
Data structures are not modified, new data structures are made instead.
This means we don't need to lock data structures. It also means that since
we don't have side effects, we make everything a separate thread and
put them together at the end.

Functional programming also allows you to use functions as arguments as the 
name implies. Functional programming is also good for generic yet typesafe
programming, but languages like OCaml can still screw this up.

Standard functional programming functions:
\begin{itemize}
\item Map - Apply a function to each element of a list and return a list made my
	concatenating the results together.
\item Fold - Track the result of calling f on each element and an accumulator,
	return the final accumulator. Can be either a left or right fold.
\end{itemize}
These are basically the Map and Reduce in MapReduce.
\subsection*{MapReduce}
Motivated by large scale data processing, parallelize across thousands of
computers, and make it easy for programmers to use. MapReduce has built-in
fault tolerance and network monitoring. The user basically has two functions
to provide
\begin{itemize}
\item map (in\_key, in\_value) -$>$ (out\_key, intermediate\_value) list -
	Input consists of records from various sources.
\item reduce (out\_key, intermediate\_value list) -$>$ out\_value list
\end{itemize}
After the map phase happens, we combine all the intermediate values for
a given key into one list and then start the reduce phase. It's good practice
for reduce to only have out\_value. We use barrier synchronization, to prevent
people from going to the reduce step while people are still mapping.

\subsubsection*{Optimization}
Each map function runs parallelly and the reduction for each key runs in parallel.
This means we have a bottleneck waiting for mapping to finish before we start
reduction. How do we optimize this?
\begin{itemize}
\item Slow-moving ``map'' tasks are run on multiple hosts and then we take the
	first one to finish.
\item Google considered making reducers lazy, but that has some design flaws
	that made it not used.
\item We can have ``combiners'' which run mini-reduce phases before the actual
	reduce in order to save bandwidth. If our reduce is both associative and
	commutative, then we can use it as our combine as well.
\end{itemize}

Written in C++, with Java and Python bindings. The dividing program tries
to divide up map tasks so that mappers are on the same computer as the data.
Tasks are chunked in 64MB which is the same size as the Google File System's
chunks.

\subsubsection*{Ensuring fault tolerance}
\begin{itemize}
\item We give up on tasks if certain key-value pairs reliably crash.
\item Mappers that fail before reporting results are redone.
\item Reducers don't claim to be done, until their results are reliably backed
	up.
\end{itemize}
\subsubsection*{Impact}
Has had great results at Google and is a shining jewel of functional programming.
Greatly simplifies large-scale computations. Lets programmer focus on problem
and let library handle messy details.

\subsection*{File Systems}
\begin{itemize}
\item A filesystem is a system to join data in a tree of files, by writing
	them to disk. Sometimes, they can support remote files or local caching.
\item Folders are the same as namespaces.
\item A ``distributed filesystem'' is one that can reach files on other machines.
\item Any decent ``distributed filesystem'' guarantees that many people can
	access many files simultaneously. It needs to be performant at the same
	time and maintain data integrity / consistency.
\end{itemize}
\subsection*{NFS}
\begin{itemize}
\item Made by Sun in the 80s. Pretty standard Unix FS. We mount remote drives
	onto local host.
\item Original was completely stateless. Higher-level protocols handle that stuff.
\item NFS defines a virtual filesystem (like an interface, not an implementation).
\item NFS locking is done with leases. Clients request locks. Servers tell them
	if that succeeds or not and take back locks that aren't renewed.
\item When we close a file, we push changes. When we open a file, we pull changes.
\item An NFS volume is managed by a single server. This makes concurrency easier,
	but puts stress on it.
\item POSIX compliance makes it portable, but very generalized.
\end{itemize}
\subsection*{GFS}
\begin{itemize}
\item Google needed a way to redundantly store data on cheap and unreliable
	systems. Also, wanted to optimize it for Googly purposes.
\item Assumptions:
	\begin{itemize}
	\item Components suck.
	\item Modest number of HUGE files.
	\item Files are write-once. Mostly appending to data.
	\item Want to read large serial chunks.
	\end{itemize}
\item Design Decisions:
	\begin{itemize}
	\item 64 MB fixed-size chunks.
	\item Each thing replicated on 3+ chunk servers.
	\item Master coordinates access and stores metadata.
	\item No data caching (files are too big).
	\item Interfaces customized for Google, but POSIX-esque.
	\end{itemize}
\item Master server is a single point of failure. We solve this by having
	backups of the master called ``shadow masters''.
\item What metadata does the master have?
	\begin{itemize}
	\item File and chunk namespaces.
	\item Mapping from files to chunks.
	\item Locations of chunk's replicas.
	\item An operations log for metadata updates.
	\end{itemize}
\item Except for the operations log, the metadata is super small and can stay in
	RAM. The operations log needs to be stored on disk for robustness reasons.
\item We can use the operations log to restore the master to a good state if
	it becomes corrupted.
\item GFS Mutation = write or append. Our goal for mutations is to minimize
	master involvement.
\item We use a lease mechanism. The master picks a replica as primary and
	gives it a lease for mutations, the primary defines a serial order for the
	changes, and the replicas follow it.
\item Atomic record append - GFS lets us append a record to a chunk atomically.
	Message is at-least-once, so we may do it multiple times, and order may
	differ between replicas.
\item So, we have a relaxed consistency model, except for our master's metadata.
	Google's fine with that, but it's something to keep in mind.
\item What are the master's responsibilities?
	\begin{itemize}
	\item Metadata storage
	\item Namespace management/locking.
	\item Monitor system health.
	\item Chunk creation, re-replication, and rebalancing.
	\item Garbage collection - We delete things by renaming them to something
		hidden, then we do the actual clean up when we're not busy.
	\item Prompt people with out of date chunks to update them.
	\end{itemize}
\item This is a highly fault tolerant system, with high availability and
	specified data integrity.
\item In conclusion, sometimes simple solutions are good and you should
	expect hard drives to break.
\section*{Clustering Algorithms}
\item Clustering is partitioning data into subsets so that the members of
	each are close in some sense.
\item Google News clusters by odds that two stories are the same story.
	We can alsop cluster hospital records, scientific images, survey data etc.
\item Some common distance metrics are Euclidean distance, Manhattan distance,
	maximum norm, but you can always define your own.
\item Clustering can either be 
	\begin{itemize}
	\item Hierarchical - building a tree (dendogram) of clusters either by
		division or by agglomeration.
	\item Partitioning - Divide set into all clusters simultaneously.
	\end{itemize}
\item K-means clustering is a form of partition clustering. This can be
	expensive and hard to do with MapReduce.
\item Canopy clustering is a preliminary step to clustering to make it easier
	to parallelize. We divide the stuff into canopies that are close. This 
	works best if our canopy distance is super cheap compared to our K-means
	distance.
\item The Elbow Criteria is a way to estimate the number of clusters in a thing.
\section*{Graph Algorithms}
\item How can we do a BFS in MapReduce? Iterate passes through MapReduce. Map
	some nodes, result include additional nodes which are fed into successive
	MapReduce passes.
\item Problem: Sending entire graph to mapper is expensive. Solution: Carefully
	consider how we represent graphs.
	\begin{itemize}
	\item The most straightforward representation is for each node to have a list of
		its neighbors. This doesn't make iteration easy and is hard to serialize.
	\item Adjacency matrix (possibly as a sparse matrix).
	\end{itemize}
\item Finding the shortest path can be done easily on one machine with Dijkstra's
	algorithm, but is highly serial. How can we parallelize this with MapReduce?
	\begin{enumerate}
	\item A map task receives $\{n : (D, points-to)\}$ $n$ is a node, $d$ away from
		start, and points to the guys in $points-to$.
	\item Map returns $(p, D+1)$ for $p$ in $points-to$.
	\item Reduce takes this and finds the minimum.
	\end{enumerate}
	Each MapReduce moves one layer further.
\item First, iteration is fast, but then we rapidly blow up. We eventually
	terminate because we don't recalculate for a node unless we find
	a faster way to it.
\item Adding edge weights its trivial.
\item Complexity is at most $n$ times as hard as Dijkstra which works out
	well if we have $n$ machines.
\subsection*{PageRank}
\item If a user clicks random links and every so often goes to a random page,
	what are the odds they will eventually be at a given page? This is used as
	a measure of site importance.
\item The MapReduce algorithm to compute this is similar to the shortest path
	where we keep track of $PR_I$ after $I$ steps.
\item PageRank doesn't necessarily converge, but gets within a few percent
	accuracy after a few hours.
\item [Phase 1:] Parse HTML - Map takes $(URL, page-content)$ and spits out
	$(URL, (PR_{init}, list-of-urls))$. Reduce is $id$.
\item [Phase 2:] PageRank Distributions - Compute PageRanks of a website's
	children. (Number of mappers = number of nodes)
\item [Phase 3:] Terminate?  If we've sufficiently converged, do so. Otherwise,
	go back to phase 2.
\item This isn't the actual algorithm used by Google, because it doesn't scale
	to the very high end.
\end{itemize}
\end{document}