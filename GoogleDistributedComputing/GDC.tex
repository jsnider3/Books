\documentclass{scrartcl}
\usepackage{fullpage}
\title{GDC MapReduce Notes}
\author{Josh Snider}
\begin{document}
\maketitle
\subsection*{FP Review}
Data structures are not modified, new data structures are made instead.
This means we don't need to lock data structures. It also means that since
we don't have side effects, we make everything a separate thread and
put them together at the end.

Functional programming also allows you to use functions as arguments as the 
name implies. Functional programming is also good for generic yet typesafe
programming, but languages like OCaml can still screw this up.

Standard functional programming functions:
\begin{itemize}
\item Map - Apply a function to each element of a list and return a list made my
	concatenating the results together.
\item Fold - Track the result of calling f on each element and an accumulator,
	return the final accumulator. Can be either a left or right fold.
\end{itemize}
These are basically the Map and Reduce in MapReduce.
\subsection*{MapReduce}
Motivated by large scale data processing, parallelize across thousands of
computers, and make it easy for programmers to use. MapReduce has built-in
fault tolerance and network monitoring. The user basically has two functions
to provide
\begin{itemize}
\item map (in\_key, in\_value) -$>$ (out\_key, intermediate\_value) list -
	Input consists of records from various sources.
\item reduce (out\_key, intermediate\_value list) -$>$ out\_value list
\end{itemize}
After the map phase happens, we combine all the intermediate values for
a given key into one list and then start the reduce phase. It's good practice
for reduce to only have out\_value. We use barrier synchronization, to prevent
people from going to the reduce step while people are still mapping.

\subsubsection*{Optimization}
Each map function runs parallelly and the reduction for each key runs in parallel.
This means we have a bottleneck waiting for mapping to finish before we start
reduction. How do we optimize this?
\begin{itemize}
\item Slow-moving ``map'' tasks are run on multiple hosts and then we take the
	first one to finish.
\item Google considered making reducers lazy, but that has some design flaws
	that made it not used.
\item We can have ``combiners'' which run mini-reduce phases before the actual
	reduce in order to save bandwidth. If our reduce is both associative and
	commutative, then we can use it as our combine as well.
\end{itemize}

Written in C++, with Java and Python bindings. The dividing program tries
to divide up map tasks so that mappers are on the same computer as the data.
Tasks are chunked in 64MB which is the same size as the Google File System's
chunks.

\subsubsection*{Ensuring fault tolerance}
\begin{itemize}
\item We give up on tasks if certain key-value pairs reliably crash.
\item Mappers that fail before reporting results are redone.
\item Reducers don't claim to be done, until their results are reliably backed
	up.
\end{itemize}
\subsubsection*{Impact}
Has had great results at Google and is a shining jewel of functional programming.
Greatly simplifies large-scale computations. Lets programmer focus on problem
and let library handle messy details.

\subsection*{File Systems}
\begin{itemize}
\item A filesystem is a system to join data in a tree of files, by writing
	them to disk. Sometimes, they can support remote files or local caching.
\item Folders are the same as namespaces.
\item A ``distributed filesystem'' is one that can reach files on other machines.
\item Any decent ``distributed filesystem'' guarantees that many people can
	access many files simultaneously. It needs to be performant at the same
	time and maintain data integrity / consistency.
\end{itemize}
\subsection*{NFS}
\begin{itemize}
\item Made by Sun in the 80s. Pretty standard Unix FS. We mount remote drives
	onto local host.
\item Original was completely stateless. Higher-level protocols handle that stuff.
\item NFS defines a virtual filesystem (like an interface, not an implementation).
\item NFS locking is done with leases. Clients request locks. Servers tell them
	if that succeeds or not and take back locks that aren't renewed.
\item When we close a file, we push changes. When we open a file, we pull changes.
\item An NFS volume is managed by a single server. This makes concurrency easier,
	but puts stress on it.
\item POSIX compliance makes it portable, but very generalized.
\end{itemize}
\subsection*{GFS}
\begin{itemize}
\item Google needed a way to redundantly store data on cheap and unreliable
	systems. Also, wanted to optimize it for Googly purposes.
\item Assumptions:
	\begin{itemize}
	\item Components suck.
	\item Modest number of HUGE files.
	\item Files are write-once. Mostly appending to data.
	\item Want to read large serial chunks.
	\end{itemize}
\item Design Decisions:
	\begin{itemize}
	\item 64 MB fixed-size chunks.
	\item Each thing replicated on 3+ chunk servers.
	\item Master coordinates access and stores metadata.
	\item No data caching (files are too big).
	\item Interfaces customized for Google, but POSIX-esque.
	\end{itemize}
\item Master server is a single point of failure. We solve this by having
	backups of the master called ``shadow masters''.
\item What metadata does the master have?
	\begin{itemize}
	\item File and chunk namespaces.
	\item Mapping from files to chunks.
	\item Locations of chunk's replicas.
	\item An operations log for metadata updates.
	\end{itemize}
\item Except for the operations log, the metadata is super small and can stay in
	RAM. The operations log needs to be stored on disk for robustness reasons.
\item We can use the operations log to restore the master to a good state if
	it becomes corrupted.
\item GFS Mutation = write or append. Our goal for mutations is to minimize
	master involvement.
\item We use a lease mechanism. The master picks a replica as primary and
	gives it a lease for mutations, the primary defines a serial order for the
	changes, and the replicas follow it.
\item Atomic record append - GFS lets us append a record to a chunk atomically.
	Message is at-least-once, so we may do it multiple times, and order may
	differ between replicas.
\item So, we have a relaxed consistency model, except for our master's metadata.
	Google's fine with that, but it's something to keep in mind.
\item What are the master's responsibilities?
	\begin{itemize}
	\item Metadata storage
	\item Namespace management/locking.
	\item Monitor system health.
	\item Chunk creation, re-replication, and rebalancing.
	\item Garbage collection - We delete things by renaming them to something
		hidden, then we do the actual clean up when we're not busy.
	\item Prompt people with out of date chunks to update them.
	\end{itemize}
\item This is a highly fault tolerant system, with high availability and
	specified data integrity.
\item In conclusion, sometimes simple solutions are good and you should
	expect hard drives to break.
\end{itemize}
\end{document}